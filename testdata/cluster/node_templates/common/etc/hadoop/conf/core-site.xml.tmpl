<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>${DEFAULT_FS}</value>
  </property>

  <property>
    <name>dfs.replication</name>
    <value>${HDFS_REPLICATION}</value>
  </property>

 <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec</value>
  </property>

  <property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>

  <property>
    <name>hadoop.proxyuser.${USER}.hosts</name>
    <value>*</value>
  </property>

  <property>
  <!-- Trash is enabled since some tests (in metadata/test_ddl.py) depend on it.
       It is set to 24 hours to avoid checkpointing between the test runs -->
    <name>fs.trash.interval</name>
    <value>1440</value>
  </property>

  <property>
  <!-- AuthorizationTest depends on auth_to_local configs. These tests are run
       irrespective of whether kerberos is enabled. -->
    <name>hadoop.security.auth_to_local</name>
    <value>RULE:[2:$1@$0](authtest@REALM.COM)s/(.*)@REALM.COM/auth_to_local_user/
RULE:[1:$1]
RULE:[2:$1]
DEFAULT</value>
  </property>

  <property>
    <name>hadoop.proxyuser.${USER}.groups</name>
    <value>*</value>
  </property>

  <!-- Needed as long as multiple nodes are running on the same address. For Impala
       testing only -->
  <property>
    <name>yarn.scheduler.include-port-in-node-name</name>
    <value>true</value>
  </property>

  <!-- Needed by frontend tests that reference s3n paths. -->
  <property>
    <name>fs.s3n.awsAccessKeyId</name>
    <value>${AWS_ACCESS_KEY_ID}</value>
  </property>

  <property>
    <name>fs.s3n.awsSecretAccessKey</name>
    <value>${AWS_SECRET_ACCESS_KEY}</value>
  </property>

  <!-- Needed when the TARGET_FILESYSTEM is s3 -->
  <property>
    <name>fs.s3a.awsAccessKeyId</name>
    <value>${AWS_ACCESS_KEY_ID}</value>
  </property>

  <property>
    <name>fs.s3a.awsSecretAccessKey</name>
    <value>${AWS_SECRET_ACCESS_KEY}</value>
  </property>

  <property>
    <name>fs.s3a.connection.maximum</name>
    <value>1500</value>
  </property>

  <!-- Location of the KMS key provider -->
  <property>
    <name>hadoop.security.key.provider.path</name>
    <value>kms://http@127.0.0.1:9600/kms</value>
  </property>

  <!-- BEGIN Kerberos settings -->
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
  </property>

  <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>*</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>*</value>
  </property>

  <!-- END Kerberos settings -->

  <property>
    <name>dfs.adls.oauth2.access.token.provider.type</name>
    <value>ClientCredential</value>
  </property>

  <property>
    <name>dfs.adls.oauth2.client.id</name>
    <value>${azure_client_id}</value>
  </property>

  <property>
    <name>dfs.adls.oauth2.credential</name>
    <value>${azure_client_secret}</value>
  </property>

  <property>
    <name>dfs.adls.oauth2.refresh.url</name>
    <value>https://login.windows.net/${azure_tenant_id}/oauth2/token</value>
  </property>

</configuration>
